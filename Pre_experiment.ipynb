{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "71Gvt2XD9xOr",
        "outputId": "f885faec-7886-41d8-92d5-4c74163f6c4e"
      },
      "outputs": [],
      "source": [
        "# Install dependencies (if not already installed)\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install opencv-python\n",
        "!pip install gradio\n",
        "!pip install transformers\n",
        "!apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "LGZIVOef689o",
        "outputId": "c921e61a-5594-4e48-9df1-618575b3b6bf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import uuid\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set device and load the CLIP model (both image and text encoders)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "def process_video(video_file, query):\n",
        "    \"\"\"\n",
        "    Process the video using CLIP's text encoder:\n",
        "      1. Encode the query text using CLIP.\n",
        "      2. Sample one frame every 5 frames from the video, and encode each frame with CLIP.\n",
        "      3. Compute cosine similarity between each frame's feature and the text feature.\n",
        "      4. Slide a 2-second window over the similarity curve, average the features in the window,\n",
        "         and compute a candidate score.\n",
        "      5. Identify the top 3 candidate segments and extract the best candidate segment via ffmpeg.\n",
        "\n",
        "    Returns:\n",
        "      - A text summary of the top 3 candidate segments (start time, end time, cosine similarity score).\n",
        "      - A plot image showing the similarity curve and highlighted candidate intervals.\n",
        "      - The best candidate video segment.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Determine the video file path\n",
        "        video_path = video_file if isinstance(video_file, str) else video_file.name\n",
        "\n",
        "        # --- Encode the query text using CLIP's text encoder ---\n",
        "        text_input = clip.tokenize([query]).to(device)\n",
        "        with torch.no_grad():\n",
        "            text_feature = model.encode_text(text_input)\n",
        "            text_feature = text_feature / text_feature.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # --- Open the video and sample every 5th frame ---\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        if fps == 0:\n",
        "            return \"Error: Unable to retrieve FPS.\", None, None\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        duration = frame_count / fps\n",
        "\n",
        "        timestamps = []\n",
        "        similarities = []\n",
        "        features_list = []\n",
        "\n",
        "        frame_index = 0\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if frame_index % 5 == 0:\n",
        "                timestamp = frame_index / fps\n",
        "                timestamps.append(timestamp)\n",
        "                # Convert frame from BGR to RGB and then to PIL image\n",
        "                image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "                image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "                with torch.no_grad():\n",
        "                    image_feature = model.encode_image(image_input)\n",
        "                    image_feature = image_feature / image_feature.norm(dim=-1, keepdim=True)\n",
        "                # Store the feature (convert to numpy array)\n",
        "                features_list.append(image_feature.cpu().numpy().squeeze())\n",
        "                # Compute cosine similarity between image feature and text feature\n",
        "                sim = (image_feature @ text_feature.T).item()\n",
        "                similarities.append(sim)\n",
        "            frame_index += 1\n",
        "        cap.release()\n",
        "\n",
        "        if not similarities:\n",
        "            return \"No features extracted from the video.\", None, None\n",
        "\n",
        "        # Convert lists to numpy arrays\n",
        "        timestamps = np.array(timestamps)\n",
        "        similarities = np.array(similarities)\n",
        "        features_array = np.array(features_list)  # shape: (num_samples, feature_dim)\n",
        "\n",
        "        # --- Sliding window candidate extraction ---\n",
        "        window_duration = 2.0  # seconds (adjustable)\n",
        "        candidates = []\n",
        "        n = len(timestamps)\n",
        "\n",
        "        # For each starting sample, define a candidate segment covering up to window_duration seconds\n",
        "        for i in range(n):\n",
        "            j = i\n",
        "            while j < n and (timestamps[j] - timestamps[i] <= window_duration):\n",
        "                j += 1\n",
        "            j = j - 1  # last valid index in the window\n",
        "            if j < i:\n",
        "                continue\n",
        "            # Average the features within the window [i, j]\n",
        "            avg_feature = np.mean(features_array[i:j+1], axis=0)\n",
        "            avg_feature = avg_feature / np.linalg.norm(avg_feature)\n",
        "            # Compute cosine similarity between the averaged feature and the CLIP text feature\n",
        "            candidate_score = np.dot(avg_feature, text_feature.cpu().numpy().squeeze())\n",
        "            candidates.append({\n",
        "                'start': timestamps[i],\n",
        "                'end': timestamps[j],\n",
        "                'score': candidate_score\n",
        "            })\n",
        "\n",
        "        # Sort candidate segments by descending score and select the top 3\n",
        "        candidates = sorted(candidates, key=lambda x: x['score'], reverse=True)\n",
        "        top3 = candidates[:3]\n",
        "\n",
        "        # Build a text summary of the top 3 candidate segments\n",
        "        output_text = \"Top 3 candidate segments:\\n\"\n",
        "        for idx, cand in enumerate(top3):\n",
        "            output_text += f\"Candidate {idx+1}: Start = {cand['start']:.2f}s, End = {cand['end']:.2f}s, Score = {cand['score']:.4f}\\n\"\n",
        "\n",
        "        if len(top3) == 0:\n",
        "            return \"No candidate segments found.\", None, None\n",
        "\n",
        "        # --- Extract the best candidate segment using ffmpeg ---\n",
        "        best_candidate = top3[0]\n",
        "        best_start = best_candidate['start']\n",
        "        best_duration = best_candidate['end'] - best_candidate['start']\n",
        "        output_video = f\"best_candidate_{uuid.uuid4().hex[:6]}.mp4\"\n",
        "        ffmpeg_cmd = f'ffmpeg -y -ss {best_start} -i \"{video_path}\" -t {best_duration} -c copy \"{output_video}\"'\n",
        "        os.system(ffmpeg_cmd)\n",
        "\n",
        "        # --- Plot the similarity curve and highlight candidate intervals ---\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(timestamps, similarities, label=\"Cosine Similarity\", marker=\"o\")\n",
        "        plt.xlabel(\"Time (s)\")\n",
        "        plt.ylabel(\"Cosine Similarity\")\n",
        "        plt.title(\"Similarity vs Time (sampled every 5 frames)\")\n",
        "        for cand in top3:\n",
        "            plt.axvspan(cand['start'], cand['end'], color='red', alpha=0.3)\n",
        "            mid = (cand['start'] + cand['end']) / 2\n",
        "            plt.text(mid, max(similarities)*0.9, f\"{cand['score']:.2f}\",\n",
        "                     ha='center', va='center', color='black')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plot_filename = f\"similarity_plot_{uuid.uuid4().hex[:6]}.png\"\n",
        "        plt.savefig(plot_filename)\n",
        "        plt.close()\n",
        "\n",
        "        return output_text, plot_filename, output_video\n",
        "    except Exception as e:\n",
        "        return f\"Error occurred: {str(e)}\", None, None\n",
        "\n",
        "# Build the Gradio interface with three outputs: candidate summary text, similarity plot image, and best candidate video.\n",
        "iface = gr.Interface(\n",
        "    fn=process_video,\n",
        "    inputs=[\n",
        "        gr.Video(label=\"Upload Video\"),\n",
        "        gr.Textbox(label=\"Query\", placeholder=\"e.g., Walking on the beach\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Candidate Scores\"),\n",
        "        gr.Image(label=\"Similarity Plot\"),\n",
        "        gr.Video(label=\"Best Candidate Segment\")\n",
        "    ],\n",
        "    title=\"CLIP Zero-Shot Video Candidate Extraction\",\n",
        "    description=(\"Upload a video and enter a text query. The system uses CLIP's text encoder to encode the query and \"\n",
        "                 \"CLIP's image encoder to extract features from video frames (every 5 frames). It computes cosine similarity, \"\n",
        "                 \"finds the top 3 candidate segments using a sliding 2-second window, and displays the candidate scores along \"\n",
        "                 \"with the best candidate video segment.\")\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyDOcaSBFOFd"
      },
      "outputs": [],
      "source": [
        "iface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
